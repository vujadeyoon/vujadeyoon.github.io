---
layout: post
title: Amazon Elastic Kubernetes Service (EKS)
date: 2022-01-28 00:00:00
description: Setting up the Amazon EKS
tags: aws, eks
categories: software
---

## Table of contents
  1. [Notice](#notice)
  2. [What is Kubernetes (K8s)?](#k8s)
  3. [What is Amazon Elastic Kubernetes Service (EKS)?](#eks)
  4. [Amazon EKS resource](#eks_resource)
  5. [How to create a cluster](#cluster)
  6. [How to create a node group](#nodegroup)
  7. [How to configure a cluster autoscaler (CA)](#ca)
  8. [How to install an ingress controller](#ingress_cont)
  9. [How to install a metric server](#metric_server)
  10. [How to apply a namespace](#namespace)
  11. [How to apply a deployment](#deployment)
  12. [How to apply a service](#service)
  13. [How to apply a horizontal pod autoscaler (HPA)](#hpa)
  14. [How to apply an ingress](#ingress)
  15. [Useful kubectl commands](#kubectl_commands)
  16. [Reference](#ref)

<hr>

## 1. Notice <a name="notice"></a>
- A guide for setting the Amazon Web Services (AWS) Cloud9
- The name of region is Asia Pacific (Seoul) and the corresponding code is ap-northeast-2 [1].
- I recommend that you should ignore the commented instructions with an octothorpe, #.
- I recommend that you should fill in the appropriate letters between the parentheses, <>.

<hr>

## 2. What is Kubernetes (K8s)? <a name="k8s"></a>
Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that
facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem.
Kubernetes services, support, and tools are widely available. The name Kubernetes originates from Greek, meaning
helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the "K" and the "s". Google
open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google's experience running production
workloads at scale with best-of-breed ideas and practices from the community [2].

<hr>

## 3. What is Amazon Elastic Kubernetes Service (EKS)? <a name="eks"></a>
Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service that makes it easy for you to run
Kubernetes on AWS and on-premises. Kubernetes is an open-source system for automating deployment, scaling, and
management of containerized applications. Amazon EKS is certified Kubernetes-conformant, so existing applications that
run on upstream Kubernetes are compatible with Amazon EKS.[3].

<hr>

## 4. Amazon EKS resource <a name="eks_resource"></a>
##### 1. Amazon EKS cluster
An Amazon EKS cluster, which calls cluster for short, consists of two primary components:
- The Amazon EKS control plane
- Amazon EKS nodes that are registered with the control plane

The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the
Kubernetes API server. The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the
Amazon EKS endpoint associated with your cluster. Each Amazon EKS cluster control plane is single-tenant and unique, and
runs on its own set of Amazon EC2 instances [4].

The kinds of the Amazon EKS cluster are public (non-private) and private cluster.
- Public cluster (non-private cluster): All node groups in the cluster are in the public subnet.
- Private cluster: All node groups in the cluster are in the private subnet. Thus, it enables that all pods (in nodes)
in the node groups are only internally accessed.

##### 2. Amazon EKS managed node group
A node group means a set of worker nodes which calls just nodes for short. A cluster consists of a number of node groups.
Please note that a cluster can be composed of various types of the node groups (e.g. g4dn.xlarge and t5.large).
There are two types of the node groups [5].
- EKS managed node group
  - Flag: managedNodeGroups
  - When creating a worker node, it supports Amazon Linux based EKS optimized Amazon Linux 2 AMI.
  - It automates the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes
    clusters [6].
- Self managed nodes
  - Flag: nodeGroups
  - When creating a worker node, it supports other operating system (OS) which is not based on Amazon Linux (e.g. Windows).

##### 3. Cluster Autosacler (CA)
The Kubernetes Cluster Autoscaler automatically adjusts the number of nodes in your cluster when pods fail or are
rescheduled onto other nodes [7].

##### 4. AWS Load Balancer Controller (ALB)
The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. Please note that The
AWS Load Balancer Controller replaces the functionality of the AWS ALB Ingress Controller for Kubernetes [8].

##### 5. Metrics Server
The Kubernetes Metrics Server is an aggregator of resource usage data in your cluster, and it is not deployed by default
in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add ons, such as the Horizontal Pod
Autoscaler (HPA) or the Kubernetes Dashboard [9].

##### 6. Namespace
A namespace is a way to group services for an application. When you create a namespace, you specify how you want to
discover service instances that you register with AWS Cloud Map: using API calls or using DNS queries. You also specify
the name that you want your application to use to discover instances [10].

##### 7. Deployment
A deployment provides declarative updates for pods and replicaset [11] In other words, the deployment manages a
replicated application on the cluster. The pods are the smallest deployable units of computing that you can create and
manage in Kubernetes [12]. The replicaset ensures that a specified number of pod replicas are running at any given time
[13].

##### 8. Service
An abstract way to expose an application running on a set of pods as a network service [14].

##### 9. Horizontal Pod Autoscaling (HPA)
In Kubernetes, a horizontal pod autoscaler automatically updates a workload resource (such as a Deployment or StatefulSet),
with the aim of automatically scaling the workload to match demand [15].
The statefulset is the workload API object used to manage stateful applications. it manages deployment and scaling of a
set of pods, with durable storage and persistent identifier for each pod [16].

##### 10. Ingress
Ingress is an Application Programming Interface (API) object that manages external access to the services in a cluster,
typically Hypertext Transfer Protocol (HTTP) [17].

<hr>

## 5. How to create a cluster <a name="cluster"></a>
##### 1. How to create a cluster
In about 40 minutes, you can create a cluster with a [cluster.yaml](/assets/blog/AWS/4_EKS/dev/cluster/cluster.yaml).
I recommend that you should check the field of the yaml file. 
- metadata:name: vujade-cluster
- metadata:region: ap-northeast-2
- metadata:version: "1.21"
- vpc:subnets:private:
  - ap-northeast-2a: { id: subnet-012a3456b78cde9f0 }
  - ap-northeast-2c: { id: subnet-0123a4bc5d678e901 }

- secretsEncryption:keyARN: arn:aws:kms:ap-northeast-2:123456789123:key/a0b1234c-de5f-6789-g01h-2i3456j789k0

In this case, the *vpc:subnets:private* is required because of the private cluster. The values of the field which can be obtained on
[VPC Dashboard](https://ap-northeast-2.console.aws.amazon.com/vpc) correspond to the subnets of the data plane which are
created in the [Amazon Virtual Private Cloud (VPC) - Section 5. How to set up subnets](/blog/2022/vpc/#subnet). Also, as far as I mentioned in
[Amazon Virtual Private Cloud (VPC) - Section 5. How to set up subnets](/blog/2022/vpc/#subnet), the considered EC2 instance type, g4dn.xlarge is only
supported ap-northeast-2a and ap-northeast-2c. The value of the *secretsEncryption:keyARN* is the MASTER_ARN which is
created in the [Amazon Web Services (AWS) Cloud9 - Section 8. How to create an AWS KMS custom managed key (CMK)](/blog/2022/cloud9/#aws_cmk). Please note that
any tags for the VPC are not required because the cluster version is 1.19+ as far as I mentioned in
[Amazon Virtual Private Cloud (VPC) - Section 4. How to create a VPC](/blog/2022/vpc/#vpc).

{% highlight yaml linenos %}
 apiVersion: eksctl.io/v1alpha5
 kind: ClusterConfig
 
 metadata:
   name: vujade-cluster
   region: ap-northeast-2
   version: "1.21"
  
 vpc:
   subnets:
     private:
       ap-northeast-2a: { id: subnet-012a3456b78cde9f0 }
       ap-northeast-2c: { id: subnet-0123a4bc5d678e901 }
 secretsEncryption:
   keyARN: arn:aws:kms:ap-northeast-2:123456789123:key/a0b1234c-de5f-6789-g01h-2i3456j789k0
{% endhighlight %}

```bash
1 $ eksctl create cluster -f ./cluster.yaml
```
```bash
    2022-01-27 06:49:03 [ℹ]  eksctl version 0.80.0
    2022-01-27 06:49:03 [ℹ]  using region ap-northeast-2
    2022-01-27 06:49:03 [✔]  using existing VPC (vpc-012abcde34fg5678h) and subnets (private:map[ap-northeast-2a:{subnet-012a3456b78cde9f0 ap-northeast-2a 10.172.92.0/23} ap-northeast-2c:{subnet-0123a4bc5d678e901 ap-northeast-2c 10.172.94.0/23}] public:map[])
    2022-01-27 06:49:03 [!]  custom VPC/subnets will be used; if resulting cluster doesn't function as expected, make sure to review the configuration of VPC/subnets
    2022-01-27 06:49:03 [ℹ]  using Kubernetes version 1.21
    2022-01-27 06:49:03 [ℹ]  creating EKS cluster "vujade-cluster" in "ap-northeast-2" region with 
    2022-01-27 06:49:03 [ℹ]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s)
    2022-01-27 06:49:03 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
    2022-01-27 06:49:03 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-2 --cluster=vujade-cluster'
    2022-01-27 06:49:03 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "vujade-cluster" in "ap-northeast-2"
    2022-01-27 06:49:03 [ℹ]  CloudWatch logging will not be enabled for cluster "vujade-cluster" in "ap-northeast-2"
    2022-01-27 06:49:03 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=vujade-cluster'
    2022-01-27 06:49:03 [ℹ]  
    2 sequential tasks: { create cluster control plane "vujade-cluster", wait for control plane to become ready 
    }
    2022-01-27 06:49:03 [ℹ]  building cluster stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:49:04 [ℹ]  deploying stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:49:34 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:50:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:51:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:52:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:53:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:54:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:55:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:56:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:57:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:58:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 06:59:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 07:00:04 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-cluster"
    2022-01-27 07:02:05 [ℹ]  waiting for the control plane availability...
    2022-01-27 07:02:05 [✔]  saved kubeconfig as "/home/ubuntu/.kube/config"
    2022-01-27 07:02:05 [ℹ]  no tasks
    2022-01-27 07:02:05 [✔]  all EKS cluster resources for "vujade-cluster" have been created
    2022-01-27 07:02:06 [ℹ]  kubectl command should work with "/home/ubuntu/.kube/config", try 'kubectl get nodes'
    2022-01-27 07:02:06 [✔]  EKS cluster "vujade-cluster" in "ap-northeast-2" region is ready
```

##### 2. How to check the created cluster
You can check the created cluster with three options.
1. You can check the cluster on [CloudFormation](https://ap-northeast-2.console.aws.amazon.com/cloudformation).
2. You can check the cluster on [Amazon Container Services - Clusters](https://ap-northeast-2.console.aws.amazon.com/eks/home?region=ap-northeast-2#/clusters).
3. You can check the cluster with an eksctl command. 
    ```bash
    1 $ eksctl get cluster
    ```
    ```bash
        2022-01-27 14:27:46 [ℹ]  eksctl version 0.80.0
        2022-01-27 14:27:46 [ℹ]  using region ap-northeast-2
        NAME            REGION          EKSCTL CREATED
        vujade-cluster  ap-northeast-2  True
    ```

##### 3. How to connect a cluster
You can connect the created cluster as follows. Please note that the field, *region* and *name* are the AWS region
and the cluster name, respectively.
```bash
1 $ aws eks --region ap-northeast-2 update-kubeconfig --name vujade-cluster
```
```bash
    Added new context arn:aws:eks:ap-northeast-2:123456789123:cluster/vujade-cluster to /home/ubuntu/.kube/config
```

##### 4. How to delete a cluster
You can delete the created cluster as follows. Please note that the field, *name* is the cluster name.
```bash
1 $ eksctl delete cluster --name vujade-cluster
```
```bash
    2022-01-27 14:27:46 [ℹ]  eksctl version 0.79.0
    2022-01-27 14:27:46 [ℹ]  using region ap-northeast-2
    2022-01-27 14:27:46 [ℹ]  deleting EKS cluster "vujade-cluster"
    2022-01-27 14:27:46 [ℹ]  deleted 0 Fargate profile(s)
    2022-01-27 14:27:46 [✔]  kubeconfig has been updated
    2022-01-18 07:43:29 [ℹ]  cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress
    2022-01-27 14:27:30 [ℹ]  1 task: { delete cluster control plane "vujade-cluster" [async] }
    2022-01-27 14:27:30 [ℹ]  will delete stack "eksctl-vujade-cluster-cluster"
    2022-01-27 14:27:30 [✔]  all cluster resources were deleted
```

<hr>

## 6. How to create a node group <a name="nodegroup"></a>
##### 1. How to create a node group
In about 20 minutes, you can create a node group with a [nodegroup.yaml](/assets/blog/AWS/4_EKS/dev/nodegroup/nodegroup.yaml).
Please note that the node group is based on the
EKS managed node group in this case. I recommend that you should check the field of the yaml file. 
- metadata:name: vujade-cluster
- metadata:region: ap-northeast-2
- managedNodeGroups:name: ng-dl
- managedNodeGroups:instanceType: g4dn.xlarge
- managedNodeGroups:minSize: 1
- managedNodeGroups:maxSize: 5
- managedNodeGroups:desiredCapacity: 1
- managedNodeGroups:volumeSize: 80
- managedNodeGroups:privateNetworking: true
- managedNodeGroups:ssh:allow: true
- managedNodeGroups:ssh:publicKeyName: vujade-eks

The fields, *managedNodeGroups:minSize*, *managedNodeGroups:maxSize* and *managedNodeGroups:desiredCapacity* are
parameters for the Cluster Autoscaler (CA) which scales up and down EC2 instances in node groups.
the *managedNodeGroups:volumeSize* means the size of the Elastic Block Storage (EBS). The default value is 80.
In this case, the *managedNodeGroups:privateNetworking* is true because the cluster is private cluster. The value of
the field, *managedNodeGroups:ssh:publicKeyName* is the name of the SSH key obtained from the
[Amazon Web Services (AWS) Cloud9 - Section 7. How to create and import a SSH key](/blog/2022/cloud9/#aws_ssh).

{% highlight yaml linenos %}
 apiVersion: eksctl.io/v1alpha5
 kind: ClusterConfig
 metadata:
   name: vujade-cluster
   region: ap-northeast-2
 managedNodeGroups:
   - name: ng-dl
     instanceType: g4dn.xlarge
     minSize: 1
     maxSize: 5
     desiredCapacity: 1
     volumeSize: 80
     privateNetworking: true
     ssh:
       allow: true
       publicKeyName: vujade-eks
{% endhighlight %}

```bash
1 $ eksctl create nodegroup -f ./eks_yaml/dev/nodegroup/nodegroup.yaml
```
```bash
    2022-01-27 07:11:22 [ℹ]  eksctl version 0.80.0
    2022-01-27 07:11:22 [ℹ]  using region ap-northeast-2
    2022-01-27 07:11:22 [ℹ]  will use version 1.21 for new nodegroup(s) based on control plane version
    2022-01-27 07:11:22 [ℹ]  nodegroup "ng-dl" will use "" [AmazonLinux2/1.21]
    2022-01-27 07:11:22 [ℹ]  using EC2 key pair "vujade-eks"
    2022-01-27 07:11:22 [ℹ]  1 nodegroup (ng-dl) was included (based on the include/exclude rules)
    2022-01-27 07:11:22 [ℹ]  will create a CloudFormation stack for each of 1 managed nodegroups in cluster "vuajde-cluster"
    2022-01-27 07:11:23 [ℹ]  
    2 sequential tasks: { fix cluster compatibility, 1 task: { 1 task: { create managed nodegroup "ng-dl" } } 
    }
    2022-01-27 07:11:23 [ℹ]  checking cluster stack for missing resources
    2022-01-27 07:11:23 [ℹ]  cluster stack has all required resources
    2022-01-27 07:11:23 [ℹ]  building managed nodegroup stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:11:23 [ℹ]  deploying stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:11:23 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:11:39 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:11:56 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:12:14 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:12:31 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:12:47 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:13:03 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:13:19 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:13:39 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:13:55 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:14:13 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:14:32 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:14:50 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:15:09 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:15:27 [ℹ]  waiting for CloudFormation stack "eksctl-vuajde-cluster-nodegroup-ng-dl"
    2022-01-27 07:15:27 [ℹ]  1 task: { install Nvidia device plugin }
    2022-01-27 07:15:27 [ℹ]  created "kube-system:DaemonSet.apps/nvidia-device-plugin-daemonset"
    2022-01-27 07:15:27 [ℹ]  as you are using the EKS-Optimized Accelerated AMI with a GPU-enabled instance type, the Nvidia Kubernetes device plugin was automatically installed.
            to skip installing it, use --install-nvidia-plugin=false.
    2022-01-27 07:15:27 [✔]  created 0 nodegroup(s) in cluster "vuajde-cluster"
    2022-01-27 07:15:27 [ℹ]  nodegroup "ng-dl" has 1 node(s)
    2022-01-27 07:15:27 [ℹ]  node "ip-10-172-95-87.ap-northeast-2.compute.internal" is ready
    2022-01-27 07:15:27 [ℹ]  waiting for at least 1 node(s) to become ready in "ng-dl"
    2022-01-27 07:15:27 [ℹ]  nodegroup "ng-dl" has 1 node(s)
    2022-01-27 07:15:27 [ℹ]  node "ip-10-172-95-87.ap-northeast-2.compute.internal" is ready
    2022-01-27 07:15:27 [✔]  created 1 managed nodegroup(s) in cluster "vuajde-cluster"
    2022-01-27 07:15:27 [ℹ]  checking security group configuration for all nodegroups
    2022-01-27 07:15:27 [ℹ]  all nodegroups have up-to-date cloudformation templates
```

##### 2. How to check the created node group
1. Check node groups. <br />
You can check node groups as follows.
```bash
1 $ eksctl get nodegroup --cluster vujade-cluster
```
```bash
    2022-01-28 00:47:46 [ℹ]  eksctl version 0.80.0
    2022-01-28 00:47:46 [ℹ]  using region ap-northeast-2
    CLUSTER         NODEGROUP       STATUS  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID        ASG NAME
    vujade-cluster  ng-dl           ACTIVE  2022-01-27T07:12:18Z    1               5               1                       g4dn.xlarge     AL2_x86_64_GPU  eks-ng-dl-0abc1d23-01a2-0a12-012a-a0bc1234d5e6
```

2. Check nodes. <br />
You can also check nodes in the node groups. Please note that you should connect the created cluster as described in
[Section 5. How to create a cluster](#cluster) before listing nodes below.
```bash
1 $ kubectl get nodes --show-labels
```
```bash
    NAME                                              STATUS   ROLES    AGE   VERSION               LABELS
    ip-10-172-95-87.ap-northeast-2.compute.internal   Ready    <none>   17h   v1.21.5-eks-9017834   alpha.eksctl.io/cluster-name=vujade-cluster,alpha.eksctl.io/nodegroup-name=ng-dl,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=g4dn.xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-04a860d4e53598406,eks.amazonaws.com/nodegroup=ng-dl,eks.amazonaws.com/sourceLaunchTemplateId=lt-0fbc06a2d297490d2,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2c,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-172-95-87.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=g4dn.xlarge,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2c
```

##### 3. How to delete a node group
You can delete a node group as follows.
```bash
1 $ eksctl delete nodegroup --region=ap-northeast-2 --cluster=vujade-cluster --name=ng-dl
```
```bash
    2022-01-18 07:41:36 [ℹ]  eksctl version 0.79.0
    2022-01-18 07:41:36 [ℹ]  using region ap-northeast-2
    2022-01-18 07:41:36 [ℹ]  1 nodegroup (ng-dl) was included (based on the include/exclude rules)
    2022-01-18 07:41:36 [ℹ]  will drain 1 nodegroup(s) in cluster "vujade-cluster"
    2022-01-18 07:41:36 [!]  no nodes found in nodegroup "ng-dl" (label selector: "alpha.eksctl.io/nodegroup-name=ng-dl")
    2022-01-18 07:41:36 [ℹ]  will delete 1 nodegroups from cluster "vujade-cluster"
    2022-01-18 07:41:36 [ℹ]  1 task: { 1 task: { delete nodegroup "ng-dl" [async] } }
    2022-01-18 07:41:36 [ℹ]  will delete stack "eksctl-vujade-cluster-nodegroup-ng-dl"
    2022-01-18 07:41:36 [ℹ]  will delete 0 nodegroups from auth ConfigMap in cluster "vujade-cluster"
    2022-01-18 07:41:36 [✔]  deleted 1 nodegroup(s) from cluster "vujade-cluster"
```

<hr>

## 7. How to configure a cluster autoscaler (CA) <a name="ca"></a>
You can configure a cluster autoscaler (CA) by executing following commands. You can get more information in the AWS EKS
workshop guide, [Configure Cluster Autoscaler (CA)](https://www.eksworkshop.com/beginner/080_scaling/deploy_ca).
Please note the CA only works when there is a node group at least.

##### 1. IAM roles for service accounts
1. Enabling IAM roles for service accounts on your cluster (i.e. Creating IAM OIDC provider).
```bash
1 $ eksctl utils associate-iam-oidc-provider --cluster vujade-cluster --approve
```
```bash
    2022-01-28 08:41:31 [ℹ]  eksctl version 0.80.0
    2022-01-28 08:41:31 [ℹ]  using region ap-northeast-2
    2022-01-28 08:41:32 [ℹ]  will create IAM Open ID Connect provider for cluster "vujade-cluster" in "ap-northeast-2"
    2022-01-28 08:41:32 [✔]  created IAM Open ID Connect provider for cluster "vujade-cluster" in "ap-northeast-2"
```

2. Creating an IAM policy for your service account that will allow your CA pod to interact with the autoscaling groups. <br />
You can create the IAM policy with [k8s-asg-policy.json](/assets/blog/AWS/4_EKS/k8s-asg-policy.json) as below. Please
note that you don't need to create the customer managed IAM policy if it already exists as below.
```bash
1 $ aws iam create-policy --policy-name k8s-asg-policy --policy-document file://./k8s-asg-policy.json
```
```bash
    An error occurred (EntityAlreadyExists) when calling the CreatePolicy operation: A policy called k8s-asg-policy already exists. Duplicate names are not allowed.
```

3. Finally, create an IAM role for the cluster-autoscaler Service Account in the kube-system namespace. <br />
You can obtain the policy ARN in [IAM - Policies](https://console.aws.amazon.com/iamv2/home#/policies) with k8s-asg-policy.
```bash
1 $ eksctl create iamserviceaccount \
2   --name cluster-autoscaler \
3   --namespace kube-system \
4   --cluster vujade-cluster \
5   --attach-policy-arn "arn:aws:iam::123456789123:policy/k8s-asg-policy" \
6   --approve \
7   --override-existing-serviceaccounts
```
```bash
    2022-01-28 09:15:51 [ℹ]  eksctl version 0.80.0
    2022-01-28 09:15:51 [ℹ]  using region ap-northeast-2
    2022-01-28 09:15:52 [ℹ]  1 iamserviceaccount (kube-system/cluster-autoscaler) was included (based on the include/exclude rules)
    2022-01-28 09:15:52 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set
    2022-01-28 09:15:52 [ℹ]  1 task: { 
        2 sequential sub-tasks: { 
            create IAM role for serviceaccount "kube-system/cluster-autoscaler",
            create serviceaccount "kube-system/cluster-autoscaler",
        } }2022-01-28 09:15:52 [ℹ]  building iamserviceaccount stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler"
    2022-01-28 09:15:52 [ℹ]  deploying stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler"
    2022-01-28 09:15:52 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler"
    2022-01-28 09:16:12 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler"
    2022-01-28 09:16:27 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-cluster-autoscaler"
    2022-01-28 09:16:27 [ℹ]  created serviceaccount "kube-system/cluster-autoscaler"
```

4. Make sure your service account with the ARN of the IAM role is annotated. <br />
```bash
1 $ kubectl -n kube-system describe sa cluster-autoscaler 
```
```bash
    Name:                cluster-autoscaler
    Namespace:           kube-system
    Labels:              app.kubernetes.io/managed-by=eksctl
    Annotations:         eks.amazonaws.com/role-arn: arn:aws:iam::123456789123:role/eksctl-vujade-cluster-addon-iamserviceaccount-Role1-1DOGJCHJ8OE45
    Image pull secrets:  <none>
    Mountable secrets:   cluster-autoscaler-token-29s2h
    Tokens:              cluster-autoscaler-token-29s2h
    Events:              <none>
```

##### 2. Deploy the Cluster Autoscaler (CA)
1. Deploy the Cluster Autoscaler to your cluster with the following command. <br />
You can get a yaml file for deploying a cluster from [official EKS workshop site](https://www.eksworkshop.com/beginner/080_scaling/deploy_ca.files/cluster-autoscaler-autodiscover.yaml)
or [my GitHub](/assets/blog/AWS/4_EKS/cluster-autoscaler-autodiscover.yaml). Please note that you should modify the
cluster name to yours.
```bash
1 $ kubectl apply -f ./cluster-autoscaler-autodiscover.yaml
```
```bash
    clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
    role.rbac.authorization.k8s.io/cluster-autoscaler created
    clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
    rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
    deployment.apps/cluster-autoscaler created
```

2. To prevent CA from removing nodes where its own pod is running, we will add the
cluster-autoscaler.kubernetes.io/safe-to-evict annotation to its deployment with the following command.
```bash
1 $ kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false"
```
```bash
    deployment.apps/cluster-autoscaler annotated
```

3. Finally let’s update the autoscaler image.
```bash
1 $ export K8S_VERSION=$(kubectl version --short | grep 'Server Version:' | sed 's/[^0-9.]*\([0-9.]*\).*/\1/' | cut -d. -f1,2)
2 $ export AUTOSCALER_VERSION=$(curl -s "https://api.github.com/repos/kubernetes/autoscaler/releases" | grep '"tag_name":' | sed -s 's/.*-\([0-9][0-9\.]*\).*/\1/' | grep -m1 ${K8S_VERSION})
3 $ kubectl -n kube-system set image deployment.apps/cluster-autoscaler cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION}
```
```bash
    deployment.apps/cluster-autoscaler image updated
```

4. You can watch logs.
```bash
1 $ kubectl -n kube-system logs -f deployment/cluster-autoscaler
```

5. Check the created cluster autoscaler.
```bash
1 $ kubectl get deploy -A 
```
```bash
    NAMESPACE     NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
    kube-system   cluster-autoscaler   1/1     1            1           7m39s
    kube-system   coredns              2/2     2            2           28h
```

<hr>

## 8. How to install an ingress controller <a name="ingress_cont"></a>
You can install an ingress controller by executing following commands. You can get more information in the 
[Ingress Controller](https://www.eksworkshop.com/beginner/130_exposing-service/ingress_controller_alb),
[AWS Load Balancer Controller](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html) and 
[Install Kubernetes Tools](https://www.eksworkshop.com/020_prerequisites/k8stools/#set-the-aws-load-balancer-controller-version).

##### 1. Deploy the AWS Load Balancer Controller
1. We will verify if the AWS Load Balancer Controller version has been set.
```bash
1 $ echo 'export LBC_VERSION="v2.3.1"' >>  ~/.bash_profile
2 $ . ~/.bash_profile
3 $ if [ ! -x ${LBC_VERSION} ]
4     then
5       tput setaf 2; echo '${LBC_VERSION} has been set.'
6     else
7       tput setaf 1; echo '${LBC_VERSION} has NOT been set.'
8   fi
```
```bash
    ${LBC_VERSION} has been set.
```

2. Create IAM OIDC provider. <br />
You can skip this step if you already execute the command in [7. How to configure a cluster autoscaler (CA)](#ca)
```bash
1 $ eksctl utils associate-iam-oidc-provider --cluster vujade-cluster --approve
```

3. Create an IAM policy, AWSLoadBalancerControllerIAMPolicy. <br />
Please note that you don't need to create the customer managed IAM policy if it already exists as below.
You can also download the [iam_policy.json](/assets/blog/AWS/4_EKS/iam_policy.json).
```bash
1 $ curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.3.1/docs/install/iam_policy.json
2 $ aws iam create-policy \
3   --policy-name AWSLoadBalancerControllerIAMPolicy \
4   --policy-document file://iam_policy.json
```
```bash
    An error occurred (EntityAlreadyExists) when calling the CreatePolicy operation: A policy called AWSLoadBalancerControllerIAMPolicy already exists. Duplicate names are not allowed.
```

4. Create a IAM role and ServiceAccount. <br />
You can obtain the policy ARN in [IAM - Policies](https://console.aws.amazon.com/iamv2/home#/policies) with AWSLoadBalancerControllerIAMPolicy.
```bash
1 $ eksctl create iamserviceaccount \
2   --cluster vujade-cluster \
3   --namespace kube-system \
4   --name aws-load-balancer-controller \
5   --attach-policy-arn arn:aws:iam::123456789123:policy/AWSLoadBalancerControllerIAMPolicy \
6   --override-existing-serviceaccounts \
7   --approve
```
```bash
    2022-01-28 13:32:15 [ℹ]  eksctl version 0.80.0
    2022-01-28 13:32:15 [ℹ]  using region ap-northeast-2
    2022-01-28 13:32:16 [ℹ]  1 existing iamserviceaccount(s) (kube-system/cluster-autoscaler) will be excluded
    2022-01-28 13:32:16 [ℹ]  1 iamserviceaccount (kube-system/aws-load-balancer-controller) was included (based on the include/exclude rules)
    2022-01-28 13:32:16 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set
    2022-01-28 13:32:16 [ℹ]  1 task: { 
        2 sequential sub-tasks: { 
            create IAM role for serviceaccount "kube-system/aws-load-balancer-controller",
            create serviceaccount "kube-system/aws-load-balancer-controller",
        } }2022-01-28 13:32:16 [ℹ]  building iamserviceaccount stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
    2022-01-28 13:32:16 [ℹ]  deploying stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
    2022-01-28 13:32:16 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
    2022-01-28 13:32:34 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
    2022-01-28 13:32:53 [ℹ]  waiting for CloudFormation stack "eksctl-vujade-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
    2022-01-28 13:32:54 [ℹ]  created serviceaccount "kube-system/aws-load-balancer-controller"
```

5. Download the IAM policy. <br />
You can also download the [iam_policy_v1_to_v2_additional.json](/assets/blog/AWS/4_EKS/iam_policy_v1_to_v2_additional.json).
```bash
1 $ curl -o iam_policy_v1_to_v2_additional.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.3.1/docs/install/iam_policy_v1_to_v2_additional.json 
```
6. Create the IAM policy and note the ARN that is returned. <br />
Please note that you don't need to create the customer managed IAM policy if it already exists as below.
```bash
1 $ aws iam create-policy \
2   --policy-name AWSLoadBalancerControllerAdditionalIAMPolicy \
3   --policy-document file://iam_policy_v1_to_v2_additional.json 
```
```bash
    An error occurred (EntityAlreadyExists) when calling the CreatePolicy operation: A policy called AWSLoadBalancerControllerAdditionalIAMPolicy already exists. Duplicate names are not allowed.
```

7. Attach the IAM policy to the IAM role. <br />
You can find the role name as follows:
    <ul>
        <li>Open the AWS CloudFormation console</li>
        <li>Select the eksctl-your-cluster-name-addon-iamserviceaccount-kube-system-aws-load-balancer-controller stack.</li>
        <li>Click the Resources tab.</li>
        <li>The role name is in the Physical ID column. bag</li>
    </ul>
```bash
1 $ aws iam attach-role-policy \
2   --role-name eksctl-vujade-cluster-addon-iamserviceaccount-Role1-1ABC2D3E4FGHI \
3   --policy-arn arn:aws:iam::123456789123:policy/AWSLoadBalancerControllerAdditionalIAMPolicy 
```

8. Add the eks-charts repository.
```bash
1 $ helm repo add eks https://aws.github.io/eks-charts
```
```bash
    "eks" has been added to your repositories
```

9. Update your local repo to make sure that you have the most recent charts.
```bash
1 $ helm repo update
```
```bash
    Hang tight while we grab the latest from your chart repositories...
    ...Successfully got an update from the "eks" chart repository
    ...Successfully got an update from the "stable" chart repository
    Update Complete. ⎈Happy Helming!⎈
```
10. Install the TargetGroupBinding CRDs.
```bash
1 $ kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"
2 $ kubectl get crd 
```
```bash
    customresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws created
    customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws created
    
    NAME                                         CREATED AT
    eniconfigs.crd.k8s.amazonaws.com             2022-01-27T06:57:16Z
    ingressclassparams.elbv2.k8s.aws             2022-01-28T13:59:28Z
    securitygrouppolicies.vpcresources.k8s.aws   2022-01-27T06:57:19Z
    targetgroupbindings.elbv2.k8s.aws            2022-01-28T13:59:28Z
```

12. Install the AWS Load Balancer Controller.
```bash
1 $ helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
2   -n kube-system \
3   --set clusterName=vujade-cluster \
4   --set serviceAccount.create=false \
5   --set serviceAccount.name=aws-load-balancer-controller
```
```bash
    NAME: aws-load-balancer-controller
    LAST DEPLOYED: Fri Jan 28 14:02:37 2022
    NAMESPACE: kube-system
    STATUS: deployed
    REVISION: 1
    TEST SUITE: None
    NOTES:
    AWS Load Balancer controller installed!
```

13. Verify that the controller is installed.
```bash
1 $ kubectl get deployment -n kube-system aws-load-balancer-controller 
```
```bash
    NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
    aws-load-balancer-controller   2/2     2            2           38s
```
<hr>

## 9. How to install a metric server <a name="metric_server"></a>
You can install a metric server which is a scalable, efficient source of container resource metrics for Kubernetes
built-in autoscaling pipelines by executing following commands. You can get more information in the 
[Configure Horizontal Pod Autoscaler (HPA)](https://www.eksworkshop.com/beginner/080_scaling/deploy_hpa).

```bash
1 $ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
```
```bash
    serviceaccount/metrics-server created
    clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
    clusterrole.rbac.authorization.k8s.io/system:metrics-server created
    rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
    clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
    clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
    service/metrics-server created
    deployment.apps/metrics-server created
    apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
```
```bash
1 $ kubectl get apiservice v1beta1.metrics.k8s.io -o json | jq '.status'
```
```bash
    {
      "conditions": [
        {
          "lastTransitionTime": "2022-01-28T14:15:04Z",
          "message": "all checks passed",
          "reason": "Passed",
          "status": "True",
          "type": "Available"
        }
      ]
    }
```
```bash
1 $ kubectl get pod -A
```
```bash
    NAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE
    kube-system   aws-load-balancer-controller-cb56f956d-cmvh7   1/1     Running   0          13m
    kube-system   aws-load-balancer-controller-cb56f956d-d76rf   1/1     Running   0          13m
    kube-system   aws-node-frdxk                                 1/1     Running   0          31h
    kube-system   cluster-autoscaler-77d7c6c9b8-8bzck            1/1     Running   0          155m
    kube-system   coredns-6dbb778559-2dhqq                       1/1     Running   0          31h
    kube-system   coredns-6dbb778559-t4twm                       1/1     Running   0          31h
    kube-system   kube-proxy-glbbl                               1/1     Running   0          31h
    kube-system   metrics-server-6dfddc5fb8-zq97v                1/1     Running   0          111s
    kube-system   nvidia-device-plugin-daemonset-q2fbv           1/1     Running   0          31h
```
<hr>

## 10. How to apply a namespace <a name="namespace"></a>
##### 1. Preparations
You can check the [namespace.yaml](/assets/blog/AWS/4_EKS/dev/namespace/namespace.yaml).

##### 2. How to apply a namespace
```bash
1 $ kubectl apply -f namespace.yaml 
```
```bash
    namespace/ns-vujade created
```

##### 3. How to check the applied namespace
```bash
1 $ kubectl get namespace
```
```bash
    NAME              STATUS   AGE
    default           Active   2d4h
    kube-node-lease   Active   2d4h
    kube-public       Active   2d4h
    kube-system       Active   2d4h
    ns-vujade         Active   50s
```

##### 4. How to delete the applied namespace
```bash
1 $ kubectl delete -f namespace.yaml
```
```bash
    namespace "ns-vujade" deleted
```

<hr>

## 11. How to apply a deployment <a name="deployment"></a>
##### 1. Preparations
You can check the [dep_dl.yaml](/assets/blog/AWS/4_EKS/dev/deployment/dep_dl.yaml).

##### 2. How to apply a deployment
```bash
1 $ kubectl apply -f deployment.yaml
```
```bash
    deployment.apps/dep-dl created
```

##### 3. How to check the applied deployment
```bash
1 $ kubectl get deployment -A
```
```bash
    NAMESPACE     NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
    kube-system   aws-load-balancer-controller   2/2     2            2           21h
    kube-system   cluster-autoscaler             1/1     1            1           23h
    kube-system   coredns                        2/2     2            2           2d4h
    kube-system   metrics-server                 1/1     1            1           21h
    ns-vujade     dep-dl                         3/3     3            3           32s
```

##### 4. How to delete the applied deployment
```bash
1 $ kubectl delete -f deployment.yaml
```
```bash
    deployment.apps "dep-dl" deleted
```

<hr>

## 12. How to apply a service <a name="service"></a>
##### 1. Preparations
You can check the [svc_dl.yaml](/assets/blog/AWS/4_EKS/dev/service/svc_dl.yaml).

##### 2. How to apply a service
```bash
1 $ kubectl apply -f service.yaml
```
```bash
    service/svc-dl created
```

##### 3. How to check the applied service
```bash
1 $ kubectl get service -A
```
```bash
    NAMESPACE     NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
    default       kubernetes                          ClusterIP   172.20.0.1       <none>        443/TCP           2d4h
    kube-system   aws-load-balancer-webhook-service   ClusterIP   172.20.122.144   <none>        443/TCP           21h
    kube-system   kube-dns                            ClusterIP   172.20.0.10      <none>        53/UDP,53/TCP     2d4h
    kube-system   metrics-server                      ClusterIP   172.20.46.230    <none>        443/TCP           21h
    ns-vujade     svc-dl                              NodePort    172.20.205.201   <none>        11001:31803/TCP   15s
```

##### 4. How to delete the applied service
```bash
1 $ kubectl delete -f service.yaml
```
```bash
    service "svc-dl" deleted
```

<hr>

## 13. How to apply a horizontal pod autoscaler (HPA) <a name="hpa"></a>
##### 1. Preparations
You can check the [hpa_dl.yaml](/assets/blog/AWS/4_EKS/dev/hpa/hpa_dl.yaml).

##### 2. How to apply a HPA
```bash
1 $ kubectl apply -f hpa.yaml
```
```bash
    horizontalpodautoscaler.autoscaling/hpa-dl created
```

##### 3. How to check the applied HPA
```bash
1 $ kubectl get hpa -A
```
```bash
    NAMESPACE   NAME     REFERENCE           TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
    ns-vujade   hpa-dl   Deployment/dep-dl   0%/65%    3         12        3          20s
```

##### 4. How to delete the applied HPA
```bash
1 $ kubectl delete -f hpa.yaml
```
```bash
    horizontalpodautoscaler.autoscaling "hpa-dl" deleted
```

<hr>

## 14. How to apply an ingress <a name="ingress"></a>
##### 1. Preparations
You can check the [ingress.yaml](/assets/blog/AWS/4_EKS/dev/ingress/ingress.yaml).

##### 2. How to apply an ingress
```bash
1 $ kubectl apply -f ingress.yaml
```
```bash
    ingress.extensions/alb-ing-eks-vujade created
```

##### 3. How to check the applied ingress
```bash
1 $ kubectl get ingress -A
```
```bash
    NAMESPACE   NAME                  CLASS    HOSTS   ADDRESS                                                                                PORTS   AGE
    ns-vujade   alb-ing-eks-vujade   <none>   *       internal-k8s-nsvujade-albingea-1716e099f7-114012471.ap-northeast-2.elb.amazonaws.com   80      54s
```

##### 4. How to delete the applied ingress
```bash
1 $ kubectl delete -f ingress.yaml
```
```bash
    ingress.extensions "alb-ing-eks-vujade" deleted
```

<hr>

## 15. Useful kubectl commands <a name="kubectl_commands"></a>
##### 1. How to list pods for a namespace
```bash
1 $ kubectl get pod --namespace ${NAMESPACE}
````

##### 2. How to watch logs for a pod  
```bash
1 $ kubectl logs -f --namespace ${NAMESPACE} ${NAME_POD}
````

##### 3. How to execute a command for a pod  
```bash
1 $ kubectl exec --namespace ${NAMESPACE} ${NAME_POD} ${COMMAND}
```

<hr>

## 16. Reference <a name="ref"></a>
1. <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html" title="Regions and Zones"> Regions and Zones</a>
2. <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes" title="What is Kubernetes?"> What is Kubernetes?</a>
3. <a href="https://aws.amazon.com/eks/features" title="Amazon EKS features"> Amazon EKS features</a>
4. <a href="https://docs.aws.amazon.com/eks/latest/userguide/clusters.html" title="Amazon EKS clusters"> Amazon EKS clusters</a>
5. <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-compute.html" title="Amazon EKS nodes"> Amazon EKS nodes</a>
6. <a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html" title="Managed node groups"> Managed node groups</a>
7. <a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html" title="Autoscaling"> Autoscaling</a>
8. <a href="https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html" title="AWS Load Balancer Controller"> AWS Load Balancer Controller</a>
9. <a href="https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html" title="Installing the Kubernetes Metrics Server"> Installing the Kubernetes Metrics Server</a>
10. <a href="https://docs.aws.amazon.com/cloud-map/latest/dg/working-with-namespaces.html" title="Working with Namespaces"> Working with Namespaces</a>
11. <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment" title="Deployments"> Deployments</a>
12. <a href="https://kubernetes.io/docs/concepts/workloads/pods" title="Pods"> Pods</a>
13. <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset" title="ReplicaSet"> ReplicaSet</a>
14. <a href="https://kubernetes.io/docs/concepts/services-networking/service" title="Service"> Service</a>
15. <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale" title="Horizontal Pod Autoscaling"> Horizontal Pod Autoscaling</a>
16. <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset" title="StatefulSets"> StatefulSets</a>
17. <a href="https://kubernetes.io/docs/concepts/services-networking/ingress" title="Ingress"> Ingress</a>

<hr>
